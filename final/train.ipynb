{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c120f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517bd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0077a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normal': 0, 'arp poisoning': 1, 'dictionary ssh': 2, 'tcp ddos': 3}\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
    "all_labels = sorted(df['label'].unique())\n",
    "attack_labels = [lbl for lbl in all_labels if lbl != 'normal']\n",
    "label_map = {'normal': 0}\n",
    "label_map.update({lbl: i + 1 for i, lbl in enumerate(attack_labels)})\n",
    "\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2703b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_encoded'] = df['label'].map(lambda x: label_map.get(x, 0)).astype(np.int64)\n",
    "\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True).fillna(0)\n",
    "feature_df = df.drop(columns=['label', 'label_encoded'], errors='ignore').select_dtypes(include=[np.number])\n",
    "labels = df['label_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4488b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['frame.len', 'ip.len', 'ip.ttl', 'ip.id', 'ip.proto', 'tcp.srcport',\n",
       "       'tcp.dstport', 'tcp.seq', 'tcp.ack', 'tcp.len', 'tcp.window_size',\n",
       "       'tcp.options.timestamp.tsval', 'udp.srcport', 'udp.dstport',\n",
       "       'udp.length', 'dns.flags.response', 'dns.flags.rcode', 'dns.qry.type',\n",
       "       'delta_time', 'delta_time__miss', 'frame.len__miss', 'ip.len__miss',\n",
       "       'ip.ttl__miss', 'tcp.seq__miss', 'tcp.ack__miss', 'tcp.len__miss',\n",
       "       'tcp.window_size__miss', 'tcp.options.timestamp.tsval__miss',\n",
       "       'udp.length__miss', 'tcp.*', 'udp.*', 'dns.*'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567fcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    feature_df, labels, test_size=0.2,\n",
    "    random_state=32, stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d421fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float32))\n",
    "x_test_scaled  = scaler.transform(x_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d48e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = x_train.shape[1]\n",
    "feature_names = list(x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0184416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NetworkEnv import IDSEnvironment\n",
    "from config import *\n",
    "\n",
    "# Use scaled numpy arrays instead of DataFrames to avoid conversion issues\n",
    "train_env = IDSEnvironment(x_train_scaled, y_train, n_features=n_features, sequence_length=SEQUENCE_LENGTH, max_steps=MAX_STEPS_PER_EPISODE)\n",
    "test_env = IDSEnvironment(x_test_scaled, y_test, n_features=n_features, sequence_length=SEQUENCE_LENGTH, max_steps=len(x_test_scaled) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e52304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def save_checkpoint(agent, episode, episode_rewards, filename):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'episode': episode,\n",
    "        'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "        'target_net_state_dict': agent.target_net.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'scheduler_state_dict': agent.scheduler.state_dict() if hasattr(agent, 'scheduler') else None,\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'steps_done': agent.steps_done,\n",
    "        'memory': agent.memory,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def load_latest_checkpoint(agent, checkpoint_dir):\n",
    "    \"\"\"Load the latest checkpoint\"\"\"\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_episode_*.pth'))\n",
    "    if not checkpoint_files:\n",
    "        print(\"No checkpoints found. Starting from scratch.\")\n",
    "        return 0, []\n",
    "    \n",
    "    # Get the latest checkpoint\n",
    "    latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_episode_')[1].split('.pth')[0]))\n",
    "    \n",
    "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    \n",
    "    # Load model states\n",
    "    agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "    agent.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if checkpoint['scheduler_state_dict'] and hasattr(agent, 'scheduler'):\n",
    "        agent.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Load training state\n",
    "    agent.steps_done = checkpoint['steps_done']\n",
    "    agent.memory = checkpoint['memory']\n",
    "    \n",
    "    start_episode = checkpoint['episode'] + 1\n",
    "    episode_rewards = checkpoint['episode_rewards']\n",
    "    \n",
    "    print(f\"Resumed from episode {start_episode}\")\n",
    "    return start_episode, episode_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cef8912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NetworkIDS\\.conda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. Starting from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Episode 1 Complete:\n",
      "Performance: Reward=-21802.00, Steps=2000, Time=88.49s\n",
      "\n",
      "\n",
      "\n",
      "TRAINING METRICS:\n",
      "Accuracy: 0.383 | Precision: 0.366 | Recall: 0.362 | F1: 0.329\n",
      "False Positive Rate (Normal): 0.202\n",
      " Per-Class Training Results:\n",
      "         normal: P=0.435, R=0.363, F1=0.396\n",
      "         arp poisoning: P=0.021, R=0.300, F1=0.039\n",
      "         dictionary ssh: P=0.530, R=0.389, F1=0.449\n",
      "         tcp ddos: P=0.476, R=0.397, F1=0.433\n",
      "\n",
      "\n",
      "\n",
      "VALIDATION METRICS:\n",
      "Accuracy: 0.950 | Precision: 0.970 | Recall: 0.765 | F1: 0.796\n",
      "False Positive Rate (Normal): 0.001 | Avg Reward: 6.27\n",
      "Samples Tested: 2000\n",
      "\n",
      "\n",
      "\n",
      "Per-Class Validation Results:\n",
      "         normal: P=0.998, R=0.945, F1=0.971\n",
      "         arp poisoning: P=1.000, R=0.185, F1=0.312\n",
      "         dictionary ssh: P=0.885, R=1.000, F1=0.939\n",
      "         tcp ddos: P=0.997, R=0.929, F1=0.961\n",
      "\n",
      "\n",
      "\n",
      "Reward Trends: Avg(10)=-21802.00, Avg(100)=-21802.00\n",
      "Training: ε=0.746, LR=0.000095, Memory=2000\n",
      "   ================================================================================\n",
      "Updating target network at episode 1\n",
      "Checkpoint saved: ./checkpoints\\checkpoint_episode_0.pth\n",
      "Checkpoint saved: ./checkpoints\\checkpoint_episode_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Episode 2 Complete:\n",
      "Performance: Reward=-17322.00, Steps=2000, Time=138.58s\n",
      "\n",
      "\n",
      "\n",
      "TRAINING METRICS:\n",
      "Accuracy: 0.470 | Precision: 0.438 | Recall: 0.468 | F1: 0.407\n",
      "False Positive Rate (Normal): 0.186\n",
      " Per-Class Training Results:\n",
      "         normal: P=0.516, R=0.465, F1=0.489\n",
      "         arp poisoning: P=0.048, R=0.462, F1=0.087\n",
      "         dictionary ssh: P=0.594, R=0.462, F1=0.520\n",
      "         tcp ddos: P=0.594, R=0.485, F1=0.534\n",
      "\n",
      "\n",
      "\n",
      "VALIDATION METRICS:\n",
      "Accuracy: 0.993 | Precision: 0.984 | Recall: 0.941 | F1: 0.960\n",
      "False Positive Rate (Normal): 0.001 | Avg Reward: 7.15\n",
      "Samples Tested: 2000\n",
      "\n",
      "\n",
      "\n",
      "Per-Class Validation Results:\n",
      "         normal: P=0.998, R=0.998, F1=0.998\n",
      "         arp poisoning: P=0.955, R=0.778, F1=0.857\n",
      "         dictionary ssh: P=0.987, R=0.999, F1=0.993\n",
      "         tcp ddos: P=0.997, R=0.990, F1=0.994\n",
      "\n",
      "\n",
      "\n",
      "Reward Trends: Avg(10)=-19562.00, Avg(100)=-19562.00\n",
      "Training: ε=0.620, LR=0.000086, Memory=4000\n",
      "   ================================================================================\n",
      "Checkpoint saved: ./checkpoints\\checkpoint_episode_1.pth\n",
      "Checkpoint saved: ./checkpoints\\checkpoint_episode_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                               "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Update step progress bar with current reward and accuracy\u001b[39;00m\n\u001b[0;32m     59\u001b[0m current_accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_predictions \u001b[38;5;28;01mif\u001b[39;00m total_predictions \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\NetworkIDS\\final\\agent.py:218\u001b[0m, in \u001b[0;36mDTQNAgent.optimize_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# 5. Optimize\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 218\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Gradient clipping for transformer stability\u001b[39;00m\n\u001b[0;32m    221\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32md:\\NetworkIDS\\.conda\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NetworkIDS\\.conda\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NetworkIDS\\.conda\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from agent import DTQNAgent\n",
    "\n",
    "# Create checkpoints directory\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize agent\n",
    "agent = DTQNAgent(n_features=n_features, n_actions=train_env.action_space.n)\n",
    "\n",
    "# Load latest checkpoint if available\n",
    "start_episode, episode_rewards = load_latest_checkpoint(agent, checkpoint_dir)\n",
    "\n",
    "for i_episode in range(start_episode, NUM_EPISODES):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    state = train_env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Track metrics for this episode\n",
    "    episode_actions = []\n",
    "    episode_true_labels = []\n",
    "    episode_rewards_list = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Progress bar for steps within each episode\n",
    "    step_pbar = tqdm(range(MAX_STEPS_PER_EPISODE), desc=f\"DTQN Episode {i_episode+1} Steps\", leave=False, unit=\"step\")\n",
    "    \n",
    "    for t in step_pbar:\n",
    "        # Select and perform an action - no need for .values since state is already numpy array\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, true_label = train_env.step(action.item())\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_actions.append(action.item())\n",
    "        episode_true_labels.append(true_label)\n",
    "        episode_rewards_list.append(reward)\n",
    "        \n",
    "        if action.item() == true_label:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Store the transition in memory - no need for DataFrame conversion\n",
    "        agent.memory.push(state, action, next_state, reward, done)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        agent.optimize_model()\n",
    "        \n",
    "        # Update step progress bar with current reward and accuracy\n",
    "        current_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        step_pbar.set_postfix({\n",
    "            \"Reward\": f\"{episode_reward:.2f}\", \n",
    "            \"Acc\": f\"{current_accuracy:.3f}\",\n",
    "            \"Step\": t+1\n",
    "        })\n",
    "        \n",
    "        if done:\n",
    "            step_pbar.close()\n",
    "            break\n",
    "    else:\n",
    "        step_pbar.close()\n",
    "            \n",
    "    episode_rewards.append(episode_reward)\n",
    "    elapsed_time = (time.time() - start_time) / 60.0\n",
    "    \n",
    "    # Calculate episode metrics\n",
    "    episode_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    # Calculate multi-class confusion matrix elements\n",
    "    n_classes = len(set(episode_true_labels + episode_actions))\n",
    "    \n",
    "    # Calculate per-class metrics for training episode\n",
    "    class_metrics = {}\n",
    "    for class_id in range(n_classes):\n",
    "        tp = sum(1 for pred, true in zip(episode_actions, episode_true_labels) if pred == class_id and true == class_id)\n",
    "        tn = sum(1 for pred, true in zip(episode_actions, episode_true_labels) if pred != class_id and true != class_id)\n",
    "        fp = sum(1 for pred, true in zip(episode_actions, episode_true_labels) if pred == class_id and true != class_id)\n",
    "        fn = sum(1 for pred, true in zip(episode_actions, episode_true_labels) if pred != class_id and true == class_id)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        class_metrics[class_id] = {\n",
    "            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "            'precision': precision, 'recall': recall, 'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    # Overall metrics (macro-average)\n",
    "    precision = np.mean([class_metrics[i]['precision'] for i in class_metrics]) if class_metrics else 0\n",
    "    recall = np.mean([class_metrics[i]['recall'] for i in class_metrics]) if class_metrics else 0\n",
    "    f1_score = np.mean([class_metrics[i]['f1_score'] for i in class_metrics]) if class_metrics else 0\n",
    "    \n",
    "    # False Positive Rate for normal class (class 0)\n",
    "    normal_fp = class_metrics.get(0, {}).get('fp', 0)\n",
    "    normal_tn = class_metrics.get(0, {}).get('tn', 0)\n",
    "    fpr = normal_fp / (normal_fp + normal_tn) if (normal_fp + normal_tn) > 0 else 0\n",
    "    \n",
    "    # Calculate epsilon value for exploration tracking\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * agent.steps_done / EPS_DECAY)\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    avg_reward_10 = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "    avg_reward_100 = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = agent.scheduler.get_last_lr()[0] if hasattr(agent.scheduler, 'get_last_lr') else LR\n",
    "    \n",
    "    # Perform quick validation with more thorough sampling\n",
    "    val_metrics = agent.quick_validation(test_env, max_samples=2000)  # Increased sample size\n",
    "    \n",
    "    print('\\n\\n') \n",
    "    # Print comprehensive episode metrics\n",
    "    print(f\"Episode {i_episode + 1} Complete:\")\n",
    "    print(f\"Performance: Reward={episode_reward:.2f}, Steps={t+1}, Time={elapsed_time:.2f}s\")\n",
    "    \n",
    "    # Training Metrics\n",
    "    print('\\n\\n') \n",
    "    print(f\"TRAINING METRICS:\")\n",
    "    print(f\"Accuracy: {episode_accuracy:.3f} | Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1_score:.3f}\")\n",
    "    print(f\"False Positive Rate (Normal): {fpr:.3f}\")\n",
    "    \n",
    "    # Show per-class performance for training\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    print(f\" Per-Class Training Results:\")\n",
    "    for class_id in class_metrics:\n",
    "        class_name = reverse_label_map.get(class_id, f\"Class_{class_id}\")\n",
    "        cm = class_metrics[class_id]\n",
    "        print(f\"         {class_name}: P={cm['precision']:.3f}, R={cm['recall']:.3f}, F1={cm['f1_score']:.3f}\")\n",
    "    \n",
    "    print('\\n\\n') \n",
    "    # Validation Metrics\n",
    "    print(f\"VALIDATION METRICS:\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.3f} | Precision: {val_metrics['precision']:.3f} | Recall: {val_metrics['recall']:.3f} | F1: {val_metrics['f1_score']:.3f}\")\n",
    "    print(f\"False Positive Rate (Normal): {val_metrics['fpr']:.3f} | Avg Reward: {val_metrics['reward']:.2f}\")\n",
    "    print(f\"Samples Tested: {val_metrics['samples']}\")\n",
    "    \n",
    "    print('\\n\\n') \n",
    "    # Show per-class performance for validation\n",
    "    print(f\"Per-Class Validation Results:\")\n",
    "    for class_id in val_metrics['class_metrics']:\n",
    "        class_name = reverse_label_map.get(class_id, f\"Class_{class_id}\")\n",
    "        cm = val_metrics['class_metrics'][class_id]\n",
    "        print(f\"         {class_name}: P={cm['precision']:.3f}, R={cm['recall']:.3f}, F1={cm['f1_score']:.3f}\")\n",
    "    \n",
    "    # Performance comparison and overfitting detection\n",
    "    acc_diff = episode_accuracy - val_metrics['accuracy']\n",
    "    f1_diff = f1_score - val_metrics['f1_score']\n",
    "    print('\\n\\n') \n",
    "    print(f\"Reward Trends: Avg(10)={avg_reward_10:.2f}, Avg(100)={avg_reward_100:.2f}\")\n",
    "    print(f\"Training: ε={eps_threshold:.3f}, LR={current_lr:.6f}, Memory={len(agent.memory)}\")\n",
    "    print(\"   \" + \"=\"*80)\n",
    "    \n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(f\"Updating target network at episode {i_episode+1}\")\n",
    "        agent.update_target_net()\n",
    "    \n",
    "    # Save checkpoint every episode\n",
    "    checkpoint_filename = os.path.join(checkpoint_dir, f'checkpoint_episode_{i_episode}.pth')\n",
    "    save_checkpoint(agent, i_episode, episode_rewards, checkpoint_filename)\n",
    "    \n",
    "print(\"\\n--- DTQN Training Complete ---\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(checkpoint_dir, 'final_model.pth')\n",
    "save_checkpoint(agent, NUM_EPISODES - 1, episode_rewards, final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f6a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
